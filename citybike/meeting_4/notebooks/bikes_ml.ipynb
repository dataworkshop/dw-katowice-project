{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This time the goal is to go through some basic Machine Learning practices. We will use data we have been working on so far.\n",
    "\n",
    "### Goal\n",
    "\n",
    "The goal for this exercise is to create a Machine Learning model that will predict total number of bike rentals on a given day.\n",
    "\n",
    "Even though, there may be more interesting things to look for in the data we have, this will be good opportunity to go through the process and then everyone should be able to work on other predictions.\n",
    "\n",
    "This doesn't really take into consideration which stations were the bikes rented from or returned to. It doesn't matter what the routes were, etc. We only care for the total number of rentals for a given day.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "We need to enrich the data and then prepare it for ML training. When this is done, we can eventually execute the models and evaluate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with initial imports we definitelly need across the whole notebook\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we load the data and have a glance at it\n",
    "wyp = pd.read_csv('../../data/wyp_2018_prepared.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wyp.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal here is to predict the total (from all stations) number of renatals for a given day. You should think which columns are helpful for that, and which are not. We need to get rid of those that do not add any useful information for this particular task. \n",
    "\n",
    "Let's get rid of those that are pretty obvious first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wyp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wyp_ml = wyp.drop(columns = ['bike_num', 'start_time', 'end_time', 'departure', 'return',\n",
    "                             'duration_sec', 'start_hour', 'start_minute', 'month_day',\n",
    "                             'duration_min', 'duration_hour', 'route', 'dep_id', 'dep_lat', 'dep_lon',\n",
    "                             'ret_id', 'ret_lat', 'ret_lon', 'distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wyp_ml.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we dropped quite a few columns. But this will be very simple use case for which the information in them is not relevant for us. You may want to double check on that though, we will see how further in the notebook.\n",
    "\n",
    "The prediction should tell us how many rentals will be there for a given day, so to train a model, we need to feed it with that information from the past. In its current form, the dataframe doesn't really show it, we need to count the rentals and add it to our set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = wyp_ml.groupby(['start_day', 'start_month'])['which_day'].count()\n",
    "sums = sums.to_frame()\n",
    "sums = pd.DataFrame(sums.to_records())\n",
    "sums = sums.rename({'which_day': 'count'}, axis=1)\n",
    "sums.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have saved now count of rentals per day, we can transform our DF so that it has a single row per day with the rentals number included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(wyp_ml))\n",
    "wyp_ml_drooped = wyp_ml.drop_duplicates()\n",
    "print(len(wyp_ml_drooped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wyp_ml_2 = wyp_ml_drooped.merge(sums, left_on=['start_day', 'start_month'], right_on=['start_day', 'start_month'], how='left')\n",
    "wyp_ml_2.reset_index(drop=True, inplace=True)\n",
    "wyp_ml_2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing that seems to might have significant impact on the rentals number is weather. There are many ways to get weather information and add it to our data frame. We're not going into any details at this point however, simply load the data provided in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv('../../data/weather.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add it to our main dataframe\n",
    "\n",
    "wyp_ml_3 = pd.merge(weather, wyp_ml_2, on='daynumber')\n",
    "wyp_ml_3.reset_index(drop=True, inplace=True)\n",
    "wyp_ml_3.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wyp_ml_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wyp_ml_3.to_csv('k_dane.csv', index=False)\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#train_set_tmp, test_set_tmp = train_test_split(wyp_ml_3, test_size=0.2, random_state=42)\n",
    "\n",
    "#train_set_tmp.to_csv('k_dane_train.csv', index=False)\n",
    "#test_set_tmp.to_csv('k_dane_test.csv', index=False)\n",
    "\n",
    "#wyp_ml_3 = train_set_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wyp_ml_3 = pd.read_csv('k_dane_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wyp_ml_3.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this moment we should drop columns with duplicated data\n",
    "\n",
    "wyp_ml_4 = wyp_ml_3.drop(columns = ['daynumber', 'which_day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running ML algorithms it is good idea to have some sort of a benchmark. We will use mean number of rentals.\n",
    "\n",
    "We also need to select a performance measure.\n",
    "\n",
    "A typical performance measure for regression problems is the Root Mean Square Error (RMSE). It gives an idea of how much error the system typically makes in its predictions, with a higher weight for large errors.\n",
    "\n",
    "There may be other better measures, but for our case the RMSE is good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = np.mean(wyp_ml_4['count'])\n",
    "print('Just using average={0} has RMSE of {1}'.format(avg, np.sqrt(np.mean((wyp_ml_4['count'] - avg)**2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a feel of your data, you can plot a histogram for each numerical value.\n",
    "# You can see there some of the values are continuous and others seem to be categorical which we will\n",
    "# further analyze in next steps.\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "wyp_ml_4.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using simple scatter you can analyze dependency between features and labels\n",
    "\n",
    "wyp_ml_4.plot(kind='scatter', x='maxtemp', y='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Test Set\n",
    "\n",
    "Let's get 20% of our dataset and save it as a test set to validate the model against when it's ready.\n",
    "\n",
    "We could simply split the sets randomly, but a better idea might be to make sure we have good representation of data in our test set. Random sampling should be fine as long as the dataset is large enough, in this case the dataset is very small and thus we need to help with the split a little. This is called stratified sampling. The dataset is divided into homogeneous groups called strata, the goal is so that the right number of instances is sampled from each stratum for our test set to be good representation of the overall data. More here: https://medium.com/@411.codebrain/train-test-split-vs-stratifiedshufflesplit-374c3dbdcc36\n",
    "\n",
    "Before running any ML models, we can predict that rain levels is significant factor for bike rentals numbers. This may not be the most relevant factor, but for that one, the distribution in the overall data is very skewed which means in a small dataset, its meaning might be easily lost.\n",
    "\n",
    "The level of rain is continuous numerical attribute we need to create some categories for.\n",
    "\n",
    "Looking at the rain histogram, we can say having 4 categories should be enough. Each category should be large enough to include good number of instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wyp_ml_4.rain.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wyp_ml_4.rain.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have following categories:\n",
    "```\n",
    "1- 0\n",
    "2- 0   >= 0.3\n",
    "3- 0.3 >= 0.6\n",
    "4- 0.6 >=\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wyp_ml_4['rain_cat'] = pd.cut(wyp_ml_4.rain, bins = [-1.0, 0.0, 0.3, 0.6, np.inf], labels = [1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wyp_ml_4.rain_cat.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Scikit-Learnâ€™s StratifiedShuffleSplit class to do stratified sampling based on the rain levels category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=44)\n",
    "for train_index, test_index in split.split(wyp_ml_4, wyp_ml_4.rain_cat):\n",
    "    strat_train_set = wyp_ml_4.loc[train_index]\n",
    "    strat_test_set = wyp_ml_4.loc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many observations are there per rain levels category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set[\"rain_cat\"].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this doesn't look too good, we have underrepresentation of two last categories. This is due to the fact, there is simply not enough data with observations that fits them. We need to change the categories:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "1- 0\n",
    "2- 0   >= 0.2\n",
    "3- 0.2 >=\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wyp_ml_4['rain_cat'] = pd.cut(wyp_ml_4.rain, bins = [-1.0, 0.0, 0.2, np.inf], labels = [1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wyp_ml_4.rain_cat.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(wyp_ml_4, wyp_ml_4.rain_cat):\n",
    "    strat_train_set = wyp_ml_4.loc[train_index]\n",
    "    strat_test_set = wyp_ml_4.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set[\"rain_cat\"].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not perfect, but quite better then it was before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can remove the rain_cat attribute to have our set in its original form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"rain_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of splitting the data is to do it by start_month column. You should check which offers better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wyp_ml_4.start_month.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "# for train_index, test_index in split.split(wyp_ml_4, wyp_ml_4.start_month):\n",
    "#     strat_train_set = wyp_ml_4.loc[train_index]\n",
    "#     strat_test_set = wyp_ml_4.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strat_test_set[\"start_month\"].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you may want to run few more visualizations on your data. Let's check how attributes correlate to each other, but first reminder of Pearson Correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example](../../data/mlst_0214.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = wyp_ml_4.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix[\"meantemp\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize = (15, 15))\n",
    "sns.heatmap(corr_matrix, annot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should further examine most promising correlations. This step, in general, is used to find out oddities in your data. Sometimes values may be capped for instance and plotting correlation scatter will reveal this. \n",
    "\n",
    "Other case could involve attributes with tail-heavy distribution for which you may want to transform them using their logharithm instead for instance. \n",
    "\n",
    "In our case let's move on. \n",
    "\n",
    "At some point having dedicated meeting about visualizations and quirks they can reveal could be good idea, though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation for ML Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to create functions to transform the data in a way that is best suited for ML algorithms. We will start with dividing the train set to predictors and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes = strat_train_set.drop(\"count\", axis=1)\n",
    "bikes_labels = strat_train_set[\"count\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bikes = wyp_ml_4.drop(\"count\", axis=1)\n",
    "#bikes_labels = wyp_ml_4[\"count\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the code below and we will describe what it does once it is ran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_attribs = ['dayofweek', 'fog', 'start_day', 'start_month', 'is_weekend', 'is_free', 'shop']\n",
    "num_attribs = ['mintemp', 'maxtemp', 'rain', 'meantemp', 'dewp', 'visib', 'mxpsd', 'sndp']\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to know which attributes are numerical and which are categorical. \n",
    "\n",
    "Then we may want to apply transformations on each of them separately or on all attributes altogether.\n",
    "\n",
    "In our case, num_pipeline includes transformations that will be applied on numerical attributes only. It will look for missing values and replace them with the median for the given feature. Then it will standardise all numerical attributes.\n",
    "\n",
    "*** What do you think should be the approach here, should be standardise, normalize (min-max scaling) values or maybe a mix of both?\n",
    "\n",
    "We have a single transformation to be applied on categorical attributes, it is one hot encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_prepared = full_pipeline.fit_transform(bikes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Remember, it is important to fir scalers to the train data only, not the full dataset or the test dataset. This is especially important so that you can easily add new data for instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(bikes_prepared.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing and Training a Model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now it is the time for what we all waited for ... let's get on with the most interesting and spectacular part - training a Machine Learning Model!!!\n",
    "\n",
    "We will start with Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(bikes_prepared, bikes_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<img src=\"https://media.giphy.com/media/LwHfaQM6AzEu44CqJO/giphy.gif\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well ... that wasn't spectacular at all.\n",
    "\n",
    "But that indeed is what we need to train a model. Anyway, let's have a look at how it went."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "bikes_predictions = lin_reg.predict(bikes_prepared)\n",
    "lin_mse = mean_squared_error(bikes_labels, bikes_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad - compared to our benchmark which was 340! But we can do better I'm sure. Decision Trees next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(bikes_prepared, bikes_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_predictions = tree_reg.predict(bikes_prepared)\n",
    "tree_mse = mean_squared_error(bikes_labels, bikes_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm ... Is this for real? No error at all? Do we have a perfect model? Well ... no. This is an example of overfitting the data, badly. How to verify this? Until you know which model to choose, you shouldn't touch the test data. Instead, you should use part of the training set for training and part for model validation.\n",
    "\n",
    "One way to do it is by using the Scikit-Learn's cross-validation feature. It randomly splits the dataset into a number of distinct subsets called folds, then it trains and evaluates the Decision Trees model as many times as many folds there are using different fold for evaluation every time and training on the remaining 9 folds. We have an array of 10 evaluation scores as the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(tree_reg, bikes_prepared, bikes_labels,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aaaand it doesn't look that good any more. In fact it seems to perform a lot worse than simple linear regression model. \n",
    "\n",
    "With cross-validation you also get standard deviation - a measure of how precise the estimate is. So the Decision Tree has a score of 183 +/- 37.\n",
    "\n",
    "Let's check the score the same way for Linear Regression, though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_scores = cross_val_score(lin_reg, bikes_prepared, bikes_labels,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, it seems that the Decision Tree is overfitting too hard and as a result it performs worse then a Linear Regression model. \n",
    "\n",
    "Let's try Random Forest now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(bikes_prepared, bikes_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_predictions = forest_reg.predict(bikes_prepared)\n",
    "forest_mse = mean_squared_error(bikes_labels, bikes_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_scores = cross_val_score(forest_reg, bikes_prepared, bikes_labels,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this is even better. Bear in mind that the score on the training set is still significantly lower than on the validation sets. This means the model is still overfitting the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost as the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgboost_reg = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42)\n",
    "xgboost_reg.fit(bikes_prepared, bikes_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_predictions = xgboost_reg.predict(bikes_prepared)\n",
    "xgboost_mse = mean_squared_error(bikes_labels, bikes_predictions)\n",
    "xgboost_rmse = np.sqrt(xgboost_mse)\n",
    "xgboost_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_scores = cross_val_score(xgboost_reg, bikes_prepared, bikes_labels,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "xgboost_rmse_scores = np.sqrt(-xgboost_scores)\n",
    "display_scores(xgboost_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tunning the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid_lr = [\n",
    "    {'fit_intercept':[True], 'normalize':[True,False], 'copy_X':[True,False]},\n",
    "    {'fit_intercept':[False], 'copy_X':[True,False]},\n",
    "]\n",
    "    \n",
    "linear_reg = LinearRegression()\n",
    "grid_search_lr = GridSearchCV(linear_reg, param_grid_lr, cv=10,\n",
    "                           scoring='neg_mean_squared_error', return_train_score=True)\n",
    "\n",
    "grid_search_lr.fit(bikes_prepared, bikes_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_lr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres_lr = grid_search_lr.cv_results_\n",
    "\n",
    "for mean_score, params in zip(cvres_lr[\"mean_test_score\"], cvres_lr[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rf = [\n",
    "    {'n_estimators': [3, 10, 100, 1000], 'max_features': [4, 16, 64]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10, 100, 1000], 'max_features': [4, 16, 64]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "grid_search_rf = GridSearchCV(forest_reg, param_grid_rf, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "\n",
    "grid_search_rf.fit(bikes_prepared, bikes_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres_rf = grid_search_rf.cv_results_\n",
    "\n",
    "for mean_score, params in zip(cvres_rf[\"mean_test_score\"], cvres_rf[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = grid_search_lr.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop(\"count\", axis=1)\n",
    "y_test = strat_test_set[\"count\"].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = grid_search_rf.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop(\"count\", axis=1)\n",
    "y_test = strat_test_set[\"count\"].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "We have defined our goal, prepared the dataset with data that is significant to predict what we're looking for. Then, we have transformed the data so that it is in a form that is readable by Machine Learning algorithms. \n",
    "\n",
    "Eventually, we have trained a few different models and evaluated their performance.\n",
    "\n",
    "This doesn't cover every aspect of a Machine Learning project yet, though. We haven't touched fine tuning yet for instance. We should get back to it next time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
